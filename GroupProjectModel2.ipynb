{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd model attempt###\n",
    "We import the same libraries as before, adding the last one for the hyperparameter tunnig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_loader is used in order to group the data into batches.\n",
    "Here, I am assuming the train_set contains only the 4-fold and complete MRI scans. We don't need the 8-fold yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32) #can also try 64 or 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count() #check number of GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just take one batch from the train_loader and check if the model is working using it. \n",
    "I also made a grid to visualise the images which hopefully will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "four_folds, full_scans = batch[0].to(device), batch[1].to(device)\n",
    "print(four_folds.shape)\n",
    "print(full_scans.shape)\n",
    "grid = torchvision.utils.make_grid(four_folds[:][0:7], nrow=2)\n",
    "\n",
    "tb.add_image(\"images\", grid)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(np.transpose(grid, (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural net architecture. The layers are the same as the original AlexNet. I have changed a lot of the hyperparameters inside the model the original values were tuned for 256x images and ours are 320x. \n",
    "\n",
    "I have also changed the number of neurons in the fully connected layers in order to get a 320x image as the output of the forward propagation. \n",
    "\n",
    "Finally, I have applied a sigmoid activation function on the last layer to make sure all values are between 0 and 1, then I multiplied that by 255 and changed the type to Int32. Essentially, I transformed all output values into pixel values. \n",
    "\n",
    "The numbers I have commented next to the layers are the lengths of the square matrix on that specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=9, stride=1, padding=4), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 192, kernel_size=9, padding=4), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 384, kernel_size=7, padding=3), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 192, kernel_size=5, padding=2), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 2, kernel_size=3, padding=1),  # 320/320\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = nn.functional.sigmoid(x)\n",
    "        #x = x * 255\n",
    "        #x = x.type(torch.int32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set this to true before strating the backprop!!!! #\n",
    "\n",
    "As long as this is false, the gradients cannot be computed. It does make the forward prop a little bit faster so I set it to false initially.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False) #set to true before starting the training!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on how long the next step takes to compute, we can get a rough idea of how long we'd have to wait for the training process. If it takes a few seconds, then the training will probably take a few hours, so we might want to look again at the architecture and decide if we wanna change something first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = AlexNet()\n",
    "network.to(device) #move the model on the GPU\n",
    "\n",
    "tb.add_graph(network, four_folds)\n",
    "tb.close()\n",
    "\n",
    "output = network(four_folds)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for mean absolute error... couldn't find it already built in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(output, target):\n",
    "    loss = torch.mean(abs(output - target))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should, in theory, return the loss for all images in the batch combined. I flattened the full_scans. I'm pretty sure it won't work. I'll have to look at the shape of the data to change it properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mae(output, torch.flatten(full_scans, 1))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will compute the gradients after backprop and return the shape of the gradient tensor for the first layer, which should be the same as the shape of the weight tensor for that layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "network.features[0].weight.grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will update all the weights based on the previously computed gradients. The algorithm I used for optimisation, Adam, basically makes sure the model will converge towards a minimum faster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.03)\n",
    "optimizer.step() \n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well so far, we're gonna try going through the whole training set once.\n",
    "The total loss should hopefully decrease from one batch to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    four_folds, full_scans = batch[0].to(device), batch[1].to(device)     #take the X and y out of the batch\n",
    "    output = network(four_folds)       #feedforward\n",
    "    loss = mae(output, full_scans)     #compute the loss\n",
    "    optimizer.zero_grad()       #set current gradients to 0\n",
    "    loss.backward()      #backpropagate\n",
    "    optimizer.step()     #update the weights\n",
    "    print('total loss: ', loss.item())\n",
    "    tb.add_scalar('Loss', loss.item(), batch)\n",
    "    tb.add_histogram('C1 Weights', network.features[0].weight, batch)\n",
    "    tb.add_histogram('C1 Bias', network.features[0].bias, batch)\n",
    "    tb.add_histogram('C1 Grad', network.features[0].weight.grad, batch)\n",
    "    \n",
    "    tb.add_histogram('C2 Weights', network.features[2].weight, batch)\n",
    "    tb.add_histogram('C2 Bias', network.features[2].bias, batch)\n",
    "    tb.add_histogram('C2 Grad', network.features[2].weight.grad, batch)\n",
    "    \n",
    "    tb.add_histogram('C3 Weights', network.features[4].weight, batch)\n",
    "    tb.add_histogram('C3 Bias', network.features[4].bias, batch)\n",
    "    tb.add_histogram('C3 Grad', network.features[4].weight.grad, batch)\n",
    "    \n",
    "    tb.add_histogram('C4 Weights', network.features[6].weight, batch)\n",
    "    tb.add_histogram('C4 Bias', network.features[6].bias, batch)\n",
    "    tb.add_histogram('C4 Grad', network.features[6].weight.grad, batch)\n",
    "    \n",
    "    tb.add_histogram('C5 Weights', network.features[8].weight, batch)\n",
    "    tb.add_histogram('C5 Bias', network.features[8].bias, batch)\n",
    "    tb.add_histogram('C5 Grad', network.features[8].weight.grad, batch)\n",
    "    \n",
    "    tb.add_histogram('C6 Weights', network.features[10].weight, batch)\n",
    "    tb.add_histogram('C6 Bias', network.features[10].bias, batch)\n",
    "    tb.add_histogram('C6 Grad', network.features[10].weight.grad, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If by some miracle we get all the way here in a reasonable amount of time, we can try running multiple epochs and seeing how low we can get the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e7636ce8d6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mfour_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_scans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m#take the X and y out of the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfull_scans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_scans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#flatten the full scans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size_list = [32, 64, 128, 256]\n",
    "lr_list = [0.003, 0.01, 0.03, 0.06, 0.1]\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "        network = AlexNet()\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "        optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "        comment = f'batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        for batch in train_loader:\n",
    "            four_folds, full_scans = batch[0].to(device), batch[1].to(device)     #take the X and y out of the batch\n",
    "            output = network(four_folds)       #feedforward\n",
    "            loss = mse_loss(output, full_scans)\n",
    "            optimizer.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer.step()     #update the weights\n",
    "            print(loss.item(), \"  \")\n",
    "            total_corect = get_num_correct(output, full_scans)\n",
    "            tb.add_scalar('Loss', loss.item(), batch)\n",
    "            tb.add_scalar('Accuracy', total_loss/batch_size, batch)\n",
    "            tb.add_histogram('C1 Weights', network.features[0].weight, batch)\n",
    "            tb.add_histogram('C1 Bias', network.features[0].bias, batch)\n",
    "            tb.add_histogram('C1 Grad', network.features[0].weight.grad, batch)\n",
    "\n",
    "            tb.add_histogram('C2 Weights', network.features[2].weight, batch)\n",
    "            tb.add_histogram('C2 Bias', network.features[2].bias, batch)\n",
    "            tb.add_histogram('C2 Grad', network.features[2].weight.grad, batch)\n",
    "\n",
    "            tb.add_histogram('C3 Weights', network.features[4].weight, batch)\n",
    "            tb.add_histogram('C3 Bias', network.features[4].bias, batch)\n",
    "            tb.add_histogram('C3 Grad', network.features[4].weight.grad, batch)\n",
    "\n",
    "            tb.add_histogram('C4 Weights', network.features[6].weight, batch)\n",
    "            tb.add_histogram('C4 Bias', network.features[6].bias, batch)\n",
    "            tb.add_histogram('C4 Grad', network.features[6].weight.grad, batch)\n",
    "\n",
    "            tb.add_histogram('C5 Weights', network.features[8].weight, batch)\n",
    "            tb.add_histogram('C5 Bias', network.features[8].bias, batch)\n",
    "            tb.add_histogram('C5 Grad', network.features[8].weight.grad, batch)\n",
    "\n",
    "            tb.add_histogram('C6 Weights', network.features[10].weight, batch)\n",
    "            tb.add_histogram('C6 Bias', network.features[10].bias, batch)\n",
    "            tb.add_histogram('C6 Grad', network.features[10].weight.grad, batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
