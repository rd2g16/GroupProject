{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework for MRI reconstruction (Autumn 2019)\n",
    "\n",
    "In this tutorial, we provide the data loader to read and process the MRI data in order to ease the difficulty of training your network. By providing this, we hope you focus more on methodology development. Please feel free to change it to suit what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    \n",
    "    \n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    \n",
    "#    img_gt = T.center_crop(T.complex_abs(img_gt), [320, 320]).unsqueeze(1)\n",
    "#    img_und = T.center_crop(T.complex_abs(img_und), [320, 320]).unsqueeze(1)\n",
    "#     rawdata_und = T.center_crop(T.complex_abs(rawdata_und), [320, 320]).unsqueeze(1)\n",
    "#     norm = T.center_crop(T.complex_abs(norm), [320, 320]).unsqueeze(1)\n",
    "#     masks.T.center_crop(T.complex_abs(masks), [320, 320]).unsqueeze(1)    \n",
    "\n",
    "    img_gt = T.center_crop(T.complex_abs(img_gt), [320, 320])\n",
    "    img_und = T.center_crop(T.complex_abs(img_und), [320, 320])\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64 ,64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),  # 320/320\n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = nn.functional.sigmoid(x)\n",
    "        #x = x * 255\n",
    "        #x = x.type(torch.cuda.int32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07518192380666733   \n",
      "9.327569961547852   \n",
      "0.06144632399082184   \n",
      "0.05618124082684517   \n",
      "0.04870830103754997   \n",
      "0.026030097156763077   \n",
      "0.03983459994196892   \n",
      "0.026752814650535583   \n",
      "0.03928736597299576   \n",
      "0.024190006777644157   \n",
      "0.18752729892730713   \n",
      "0.04776955395936966   \n",
      "0.030598556622862816   \n",
      "0.03932809829711914   \n",
      "0.029066724702715874   \n",
      "0.03421145677566528   \n",
      "0.02866198867559433   \n",
      "0.020785290747880936   \n",
      "0.03168277069926262   \n",
      "0.027459660544991493   \n",
      "0.01922103948891163   \n",
      "0.026630135253071785   \n",
      "0.03134298324584961   \n",
      "0.023488536477088928   \n",
      "0.023812556639313698   \n",
      "0.024823004379868507   \n",
      "0.023182714357972145   \n",
      "0.017839733511209488   \n",
      "0.020840751007199287   \n",
      "0.02427021414041519   \n",
      "0.029289713129401207   \n",
      "0.021374840289354324   \n",
      "0.01830027624964714   \n",
      "0.022052261978387833   \n",
      "0.019012024626135826   \n",
      "0.028029121458530426   \n",
      "0.019736481830477715   \n",
      "0.018501287326216698   \n",
      "0.019669819623231888   \n",
      "0.017037028446793556   \n",
      "0.024067388847470284   \n",
      "0.021896136924624443   \n",
      "0.02141919545829296   \n",
      "0.018785448744893074   \n",
      "0.02376019023358822   \n",
      "0.0226029884070158   \n",
      "0.015987860038876534   \n",
      "0.012943347916007042   \n",
      "0.016239376738667488   \n",
      "0.017074409872293472   \n",
      "0.014565875753760338   \n",
      "0.016169538721442223   \n",
      "0.0185176320374012   \n",
      "0.017397485673427582   \n",
      "0.0205367561429739   \n",
      "0.019549483433365822   \n",
      "0.017913535237312317   \n",
      "0.017053604125976562   \n",
      "0.021685533225536346   \n",
      "0.024255428463220596   \n",
      "0.019045861437916756   \n",
      "0.019620690494775772   \n",
      "0.02333749644458294   \n",
      "0.016131307929754257   \n",
      "0.0161129217594862   \n",
      "0.017901044338941574   \n",
      "0.02540016733109951   \n",
      "0.011402934789657593   \n",
      "0.01780707575380802   \n",
      "0.016992274671792984   \n",
      "0.013213692232966423   \n",
      "0.01705477014183998   \n",
      "0.02113998681306839   \n",
      "0.013963981531560421   \n",
      "0.01608940213918686   \n",
      "0.0205954872071743   \n",
      "0.016324933618307114   \n",
      "0.02163868583738804   \n",
      "0.017260266467928886   \n",
      "0.0157996267080307   \n",
      "0.020099250599741936   \n",
      "0.014477051794528961   \n",
      "0.015041959472000599   \n",
      "0.017157960683107376   \n",
      "0.01415038201957941   \n",
      "0.02241467498242855   \n",
      "0.01433289423584938   \n",
      "0.01965092308819294   \n",
      "0.0180865116417408   \n",
      "0.01659761741757393   \n",
      "0.01896027848124504   \n",
      "0.015924157574772835   \n",
      "0.01728099212050438   \n",
      "0.015027887187898159   \n",
      "0.013680474832654   \n",
      "0.01287967897951603   \n",
      "0.014803471975028515   \n",
      "0.018996918573975563   \n",
      "0.01898159272968769   \n",
      "0.017388613894581795   \n",
      "0.018449677154421806   \n",
      "0.01785445585846901   \n",
      "0.025464490056037903   \n",
      "0.02118018828332424   \n",
      "0.01589243859052658   \n",
      "0.01344908494502306   \n",
      "0.01422971859574318   \n",
      "0.019469905644655228   \n",
      "0.016167495399713516   \n",
      "0.017357200384140015   \n",
      "0.020361045375466347   \n",
      "0.014688665047287941   \n",
      "0.01617540791630745   \n",
      "0.015984421595931053   \n",
      "0.015053893439471722   \n",
      "0.010717509314417839   \n",
      "0.012279930524528027   \n",
      "0.013872813433408737   \n",
      "0.017957424744963646   \n",
      "0.012660386972129345   \n",
      "0.013697315938770771   \n",
      "0.012364531867206097   \n",
      "0.018017631024122238   \n",
      "0.011353031732141972   \n",
      "0.014090144075453281   \n",
      "0.014003399759531021   \n",
      "0.010570315644145012   \n",
      "0.01800609938800335   \n",
      "0.014277029782533646   \n",
      "0.013572845607995987   \n",
      "0.012866998091340065   \n",
      "0.013538671657443047   \n",
      "0.01717924326658249   \n",
      "0.015173839405179024   \n",
      "0.014828120358288288   \n",
      "0.01668509654700756   \n",
      "0.012584253214299679   \n",
      "0.0172285046428442   \n",
      "0.011892545968294144   \n",
      "0.016410212963819504   \n",
      "0.012159439735114574   \n",
      "0.01202394813299179   \n",
      "0.012120145373046398   \n",
      "0.011505216360092163   \n",
      "0.015035783872008324   \n",
      "0.015840858221054077   \n",
      "0.01724994368851185   \n",
      "0.012693393975496292   \n",
      "0.017370298504829407   \n",
      "0.011944102123379707   \n",
      "0.014809859916567802   \n",
      "0.01222143042832613   \n",
      "0.012053899466991425   \n",
      "0.012699027545750141   \n",
      "0.012086687609553337   \n",
      "0.01642497628927231   \n",
      "0.013237270526587963   \n",
      "0.0156820397824049   \n",
      "0.012368815951049328   \n",
      "0.01392337679862976   \n",
      "0.013468235731124878   \n",
      "0.011278423480689526   \n",
      "0.015295696444809437   \n",
      "0.012059512548148632   \n",
      "0.011174600571393967   \n",
      "0.01722794584929943   \n",
      "0.01248446200042963   \n",
      "0.012652106583118439   \n",
      "0.011056407354772091   \n",
      "0.015383902005851269   \n",
      "0.011745559051632881   \n",
      "0.01162677351385355   \n",
      "0.012544138357043266   \n",
      "0.012676967307925224   \n",
      "0.01083382684737444   \n",
      "0.010408978909254074   \n",
      "0.018708813935518265   \n",
      "0.013853810727596283   \n",
      "0.012316501699388027   \n",
      "0.01408936083316803   \n",
      "0.011222951114177704   \n",
      "0.012002591043710709   \n",
      "0.010336173698306084   \n",
      "0.014061137102544308   \n",
      "0.01322315726429224   \n",
      "0.017436843365430832   \n",
      "0.016600556671619415   \n",
      "0.012136751785874367   \n",
      "0.016246093437075615   \n",
      "0.015168878249824047   \n",
      "0.011618124321103096   \n",
      "0.013576948083937168   \n",
      "0.012297644279897213   \n",
      "0.01080999430269003   \n",
      "0.013381395488977432   \n",
      "0.011028546839952469   \n",
      "0.01281376276165247   \n",
      "0.01208763662725687   \n",
      "0.01490592397749424   \n",
      "0.012889634817838669   \n",
      "0.01500078197568655   \n",
      "0.016076955944299698   \n",
      "0.014628549106419086   \n",
      "0.01270314957946539   \n",
      "0.010016289539635181   \n",
      "0.011547324247658253   \n",
      "0.013450047932565212   \n",
      "0.0122213838621974   \n",
      "0.010326600633561611   \n",
      "0.018701903522014618   \n",
      "0.012484858743846416   \n",
      "0.011323140934109688   \n",
      "0.013797711580991745   \n",
      "0.011280187405645847   \n",
      "0.009525645524263382   \n",
      "0.00893932394683361   \n",
      "0.012074464932084084   \n",
      "0.012689088471233845   \n",
      "0.014056898653507233   \n",
      "0.012111498974263668   \n",
      "0.008858127519488335   \n",
      "0.012486062943935394   \n",
      "0.011259134858846664   \n",
      "0.015386038459837437   \n",
      "0.014850541949272156   \n",
      "0.016652390360832214   \n",
      "0.011214714497327805   \n",
      "0.011521070264279842   \n",
      "0.010790000669658184   \n",
      "0.010021289810538292   \n",
      "0.011865872889757156   \n",
      "0.012669024989008904   \n",
      "0.008167561143636703   \n",
      "0.01535725686699152   \n",
      "0.013815676793456078   \n",
      "0.015008468180894852   \n",
      "0.014728650450706482   \n",
      "0.011376596055924892   \n",
      "0.011581583879888058   \n",
      "0.014628807082772255   \n",
      "0.012001890689134598   \n",
      "0.010886252857744694   \n",
      "0.01150318793952465   \n",
      "0.012200922705233097   \n",
      "0.011932562105357647   \n",
      "0.013946783728897572   \n",
      "0.010666797868907452   \n",
      "0.012721995823085308   \n",
      "0.014830261468887329   \n",
      "0.01175056304782629   \n",
      "0.009901577606797218   \n",
      "0.01575251668691635   \n",
      "0.017984652891755104   \n",
      "0.011340745724737644   \n",
      "0.012825005687773228   \n",
      "0.013620148412883282   \n",
      "0.012690456584095955   \n",
      "0.01421996671706438   \n",
      "0.010625578463077545   \n",
      "0.012314616702497005   \n",
      "0.010497148148715496   \n",
      "0.010579854249954224   \n",
      "0.010962854139506817   \n",
      "0.010521323420107365   \n",
      "0.01029499713331461   \n",
      "0.009253292344510555   \n",
      "0.009892831556499004   \n",
      "0.010465637780725956   \n",
      "0.010645508766174316   \n",
      "0.012976461090147495   \n",
      "0.010945666581392288   \n",
      "0.01490172278136015   \n",
      "0.010163087397813797   \n",
      "0.013418368995189667   \n",
      "0.013063209131360054   \n",
      "0.011967450380325317   \n",
      "0.01107895839959383   \n",
      "0.014168918132781982   \n",
      "0.010329310782253742   \n",
      "0.01018417626619339   \n",
      "0.008026925846934319   \n",
      "0.011583839543163776   \n",
      "0.010443052276968956   \n",
      "0.012398059479892254   \n",
      "0.013200941495597363   \n",
      "0.01318629365414381   \n",
      "0.012348214164376259   \n",
      "0.009711074642837048   \n",
      "0.01073696929961443   \n",
      "0.011622640304267406   \n",
      "0.010053448379039764   \n",
      "0.011817378923296928   \n",
      "0.013227245770394802   \n",
      "0.0123831108212471   \n",
      "0.011079258285462856   \n",
      "0.009608007036149502   \n",
      "0.011347389779984951   \n",
      "0.01074804924428463   \n",
      "0.012132687494158745   \n",
      "0.013331879861652851   \n",
      "0.008783360943198204   \n",
      "0.01048640999943018   \n",
      "0.0096922367811203   \n",
      "0.013059898279607296   \n",
      "0.00934743881225586   \n",
      "0.010883900336921215   \n",
      "0.010094530880451202   \n",
      "0.012551629915833473   \n",
      "0.011596424505114555   \n",
      "0.01079416461288929   \n",
      "0.011758220382034779   \n",
      "0.010067443363368511   \n",
      "0.01193834189325571   \n",
      "0.010005059652030468   \n",
      "0.011019927449524403   \n",
      "0.010202578268945217   \n",
      "0.009572716429829597   \n",
      "0.011315513402223587   \n",
      "0.012225138023495674   \n",
      "0.008355678990483284   \n",
      "0.01086324080824852   \n",
      "0.011428866535425186   \n",
      "0.012918609194457531   \n",
      "0.010968967340886593   \n",
      "0.011379040777683258   \n",
      "0.011853178963065147   \n",
      "0.010143923573195934   \n",
      "0.0134684843942523   \n",
      "0.009938753210008144   \n",
      "0.009532179683446884   \n",
      "0.01011462602764368   \n",
      "0.01136569119989872   \n",
      "0.010644417256116867   \n",
      "0.010563426651060581   \n",
      "0.011995331384241581   \n",
      "0.010170170105993748   \n",
      "0.010669118724763393   \n",
      "0.008794778026640415   \n",
      "0.012253440916538239   \n",
      "0.011699401773512363   \n",
      "0.011012383736670017   \n",
      "0.009032566100358963   \n",
      "0.010675284080207348   \n",
      "0.011284382082521915   \n",
      "0.014437113888561726   \n",
      "0.012069758959114552   \n",
      "0.010098404251039028   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011401934549212456   \n",
      "0.0119841443374753   \n",
      "0.009917051531374454   \n",
      "0.010840573348104954   \n",
      "0.011116345413029194   \n",
      "0.008670862764120102   \n",
      "0.010109918192029   \n",
      "0.01038879994302988   \n",
      "0.01097245141863823   \n",
      "0.00920596532523632   \n",
      "0.011301460675895214   \n",
      "0.009846222586929798   \n",
      "0.00863924901932478   \n",
      "0.010996202938258648   \n",
      "0.011646910570561886   \n",
      "0.010230579413473606   \n",
      "0.011165080592036247   \n",
      "0.009015867486596107   \n",
      "0.008371284231543541   \n",
      "0.009074992500245571   \n",
      "0.009173386730253696   \n",
      "0.007803079206496477   \n",
      "0.011048378422856331   \n",
      "0.009998255409300327   \n",
      "0.007006182800978422   \n",
      "0.009458058513700962   \n",
      "0.016204319894313812   \n",
      "0.014772916212677956   \n",
      "0.010573266074061394   \n",
      "0.009375123307108879   \n",
      "0.011076550930738449   \n",
      "0.011700423434376717   \n",
      "0.01072125043720007   \n",
      "0.009597443975508213   \n",
      "0.009798936545848846   \n",
      "0.012078200466930866   \n",
      "0.00984059926122427   \n",
      "0.010668319649994373   \n",
      "0.008705221116542816   \n",
      "0.01007887627929449   \n",
      "0.011641093529760838   \n",
      "0.009905527345836163   \n",
      "0.009391659870743752   \n",
      "0.009806104004383087   \n",
      "0.012389786541461945   \n",
      "0.011762918904423714   \n",
      "0.010573286563158035   \n",
      "0.009383369237184525   \n",
      "0.008975593373179436   \n",
      "0.009588619694113731   \n",
      "0.009950311854481697   \n",
      "0.009697964414954185   \n",
      "0.010139367543160915   \n",
      "0.01037061121314764   \n",
      "0.010430256836116314   \n",
      "0.008114125579595566   \n",
      "0.008998380973935127   \n",
      "0.010134330950677395   \n",
      "0.009720307774841785   \n",
      "0.009045028127729893   \n",
      "0.007990358397364616   \n",
      "0.012739014811813831   \n",
      "0.008130041882395744   \n",
      "0.009564750827848911   \n",
      "0.00802814494818449   \n",
      "0.009716219268739223   \n",
      "0.012875351123511791   \n",
      "0.008185185492038727   \n",
      "0.008632310666143894   \n",
      "0.010784707963466644   \n",
      "0.008905074559152126   \n",
      "0.007810599636286497   \n",
      "0.00995760690420866   \n",
      "0.010105851106345654   \n",
      "0.009579751640558243   \n",
      "0.009173558093607426   \n",
      "0.008442922495305538   \n",
      "0.011337540112435818   \n",
      "0.011126119643449783   \n",
      "0.011173831298947334   \n",
      "0.011403490789234638   \n",
      "0.009657872840762138   \n",
      "0.009512938559055328   \n",
      "0.01141862291842699   \n",
      "0.010099370032548904   \n",
      "0.010217401199042797   \n",
      "0.010293153114616871   \n",
      "0.009817682206630707   \n",
      "0.009070581756532192   \n",
      "0.010440205223858356   \n",
      "0.010436221025884151   \n",
      "0.010912983678281307   \n",
      "0.00827722903341055   \n",
      "0.009953884407877922   \n",
      "0.008571913465857506   \n",
      "0.009868606925010681   \n",
      "0.010566161014139652   \n",
      "0.008869308978319168   \n",
      "0.009690063074231148   \n",
      "0.0116647407412529   \n",
      "0.008806319907307625   \n",
      "0.009631230495870113   \n",
      "0.00954960472881794   \n",
      "0.009400634095072746   \n",
      "0.008724616840481758   \n",
      "0.012035360559821129   \n",
      "0.010431296192109585   \n",
      "0.00996175967156887   \n",
      "0.007361696567386389   \n",
      "0.011385688558220863   \n",
      "0.013851209543645382   \n",
      "0.01179590169340372   \n",
      "0.009312797337770462   \n",
      "0.008897961117327213   \n",
      "0.008274518884718418   \n",
      "0.010864713229238987   \n",
      "0.009778994135558605   \n",
      "0.01097453385591507   \n",
      "0.008409714326262474   \n",
      "0.008462472818791866   \n",
      "0.007868742570281029   \n",
      "0.00941732618957758   \n",
      "0.014025230892002583   \n",
      "0.011340315453708172   \n",
      "0.010870329104363918   \n",
      "0.008435098454356194   \n",
      "0.01028422825038433   \n",
      "0.00995371863245964   \n",
      "0.010000159963965416   \n",
      "0.009123057126998901   \n",
      "0.009753542020916939   \n",
      "0.010339058935642242   \n",
      "0.00935368798673153   \n",
      "0.007950818166136742   \n",
      "0.011443073861300945   \n",
      "0.008578294888138771   \n",
      "0.008327544666826725   \n",
      "0.009374667890369892   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = '/tmp/NC2019MRI/train'\n",
    "    data_path_val = '/tmp/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    lr = 1e-2\n",
    "    \n",
    "    network = AlexNet()\n",
    "    network.to('cuda:0') #move the model on the GPU\n",
    "    mse_loss = nn.SmoothL1Loss().to('cuda:0')\n",
    "    \n",
    "    optimizer = optim.Adagrad(network.parameters(), lr=lr)\n",
    "    \n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=22, num_workers=num_workers) \n",
    "    \n",
    "\n",
    "    for epoch in range(5):\n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "        \n",
    "            img_gt, img_und = sample\n",
    "        \n",
    "            img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "            img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "\n",
    "            output = network(img_und)       #feedforward\n",
    "            #output = output.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "            loss = mse_loss(output, img_gt)\n",
    "            #loss = torch.tensor(ssim(img_gt, output)).to('cuda:0')\n",
    "            #print(loss.item())\n",
    "            optimizer.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer.step()     #update the weights\n",
    "\n",
    "            print(loss.item(), \"  \")\n",
    "        \n",
    "\n",
    "        \n",
    "        # stack different slices into a volume for visualisation\n",
    "#         A = masks[...,0].squeeze()\n",
    "#         B = torch.log(T.complex_abs(rawdata_und) + 1e-9).squeeze()\n",
    "#         C = T.complex_abs(img_und).squeeze()\n",
    "#         D = T.complex_abs(img_gt).squeeze()\n",
    "#         all_imgs = torch.stack([A,B,C,D], dim=0)\n",
    "\n",
    "#         # from left to right: mask, masked kspace, undersampled image, ground truth\n",
    "#         show_slices(all_imgs, [0, 1, 2, 3], cmap='gray')\n",
    "#         plt.pause(1)\n",
    "\n",
    "#         if iteration >= 0: break  # show 4 random slices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6400861740112305\n",
      "0.340620756149292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2134"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, gt = train_dataset[3]\n",
    "image = image.unsqueeze(0).to('cuda:0')\n",
    "image = image.unsqueeze(0)\n",
    "#gt = gt.unsqueeze(0).to('cuda:0')\n",
    "gt = gt.unsqueeze(0).numpy()\n",
    "output = network(image)\n",
    "output = output.squeeze(1).cpu().detach().numpy()\n",
    "loss = torch.tensor(ssim(gt, output))\n",
    "loss2 = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "print(loss.item())\n",
    "print(loss2.item())\n",
    "#loss2 = mse_loss(output, gt)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13082477027915188\n",
      "0.4987707764482878\n"
     ]
    }
   ],
   "source": [
    "e = []\n",
    "a=[]\n",
    "b=[]\n",
    "i = 0\n",
    "for i in range(0,len(train_dataset)):\n",
    "    image, gt = train_dataset[i]\n",
    "    image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    #gt = gt.unsqueeze(0).to('cuda:0')\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network(image)\n",
    "    output = output.squeeze(1).cpu().detach().numpy()\n",
    "    loss = torch.tensor(ssim(gt, output))\n",
    "    loss2 = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    e.append(loss.item()-loss2.item())\n",
    "    a.append(loss.item())\n",
    "\n",
    "    if loss.item()-loss2.item() < 0:\n",
    "        i+=1\n",
    "print(np.nanmean(e))\n",
    "print(np.nanmean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011205554008483887\n",
      "0.0060116928070783615\n"
     ]
    }
   ],
   "source": [
    "image, gt = train_dataset[3]\n",
    "image = image.unsqueeze(0).to('cuda:0')\n",
    "image = image.unsqueeze(0)\n",
    "gt = gt.unsqueeze(0).to('cuda:0')\n",
    "gt = gt.unsqueeze(0)\n",
    "output = network(image)\n",
    "loss = mse_loss(output, gt)\n",
    "loss2 = mse_loss(image, gt)\n",
    "print(loss.item())\n",
    "print(loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 8\n",
    "cen_fract = 0.04\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = '/tmp/NC2019MRI/train'\n",
    "    data_path_val = '/tmp/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    # create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "\n",
    "    a = [[],[]]\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "        img_gt = T.center_crop(T.complex_abs(img_gt), [320, 320]).unsqueeze(1)\n",
    "        img_und = T.center_crop(T.complex_abs(img_und), [320, 320]).unsqueeze(1)\n",
    "        a[0].append(img_und)\n",
    "        a[1].append(img_gt)\n",
    "    b = torch.cat(a[0][:])\n",
    "    c = torch.cat(a[1][:])\n",
    "train = torch.stack((b,c),dim=0)\n",
    "del a\n",
    "del b\n",
    "del c\n",
    "del train_loader\n",
    "del train_dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b26a81dc5f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawdata_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m#feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "    \n",
    "network = AlexNet()\n",
    "network.to('cuda:0') #move the model on the GPU\n",
    "mse_loss = nn.MSELoss().to('cuda:0')\n",
    "    \n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "for iteration, sample in enumerate(train_loader):\n",
    "    #img_gt, img_und, rawdata_und, masks, norm = sample        \n",
    "    \n",
    "    output = network(img_und)       #feedforward\n",
    "    print(output.shape)\n",
    "\n",
    "    loss = mse_loss(output, img_gt)\n",
    "    optimizer.zero_grad()       #set current gradients to 0\n",
    "    loss.backward()      #backpropagate\n",
    "    optimizer.step()     #update the weights\n",
    "    print(loss.item(), \"  \")\n",
    "        \n",
    "    i = 0\n",
    "    j +=1\n",
    "        \n",
    "    if j%100 == 0:\n",
    "        for row in range(0,320):\n",
    "            for col in range(0,320):\n",
    "                if output[0,0,row,col].item() == img_gt[0,0,row,col].item():\n",
    "\n",
    "                        i +=1\n",
    "        print(i, \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-33ad8426963b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2])\n",
    "a.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
