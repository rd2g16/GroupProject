{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework for MRI reconstruction (Autumn 2019)\n",
    "\n",
    "In this tutorial, we provide the data loader to read and process the MRI data in order to ease the difficulty of training your network. By providing this, we hope you focus more on methodology development. Please feel free to change it to suit what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    \n",
    "    \n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "    \n",
    "#    img_gt = T.center_crop(T.complex_abs(img_gt), [320, 320]).unsqueeze(1)\n",
    "#    img_und = T.center_crop(T.complex_abs(img_und), [320, 320]).unsqueeze(1)\n",
    "#     rawdata_und = T.center_crop(T.complex_abs(rawdata_und), [320, 320]).unsqueeze(1)\n",
    "#     norm = T.center_crop(T.complex_abs(norm), [320, 320]).unsqueeze(1)\n",
    "#     masks.T.center_crop(T.complex_abs(masks), [320, 320]).unsqueeze(1)    \n",
    "\n",
    "    img_gt = T.center_crop(T.complex_abs(img_gt), [320, 320])\n",
    "    img_und = T.center_crop(T.complex_abs(img_und), [320, 320])\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),  # 320/320\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = nn.functional.sigmoid(x)\n",
    "        #x = x * 255\n",
    "        #x = x.type(torch.cuda.int32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15101312100887299   \n",
      "1.4425983428955078   \n",
      "1.4402141571044922   \n",
      "0.15985983610153198   \n",
      "0.1011737659573555   \n",
      "0.050378698855638504   \n",
      "0.02328074537217617   \n",
      "0.032161399722099304   \n",
      "0.0170602947473526   \n",
      "0.015053101815283298   \n",
      "0.014114351943135262   \n",
      "0.014127738773822784   \n",
      "0.009700944647192955   \n",
      "0.014771457761526108   \n",
      "0.012811270542442799   \n",
      "0.011615210212767124   \n",
      "0.011751163750886917   \n",
      "0.01390205230563879   \n",
      "0.012319824658334255   \n",
      "0.010012771002948284   \n",
      "0.0144766541197896   \n",
      "0.021094605326652527   \n",
      "0.013093112036585808   \n",
      "0.011681227013468742   \n",
      "0.01188849750906229   \n",
      "0.0196065753698349   \n",
      "0.011501949280500412   \n",
      "0.01778835989534855   \n",
      "0.010597818531095982   \n",
      "0.016620924696326256   \n",
      "0.016758093610405922   \n",
      "0.01882094517350197   \n",
      "0.009998257271945477   \n",
      "0.013137007132172585   \n",
      "0.0184067040681839   \n",
      "0.011674619279801846   \n",
      "0.01126742921769619   \n",
      "0.009039469063282013   \n",
      "0.012987323105335236   \n",
      "0.01338907890021801   \n",
      "0.012886724434792995   \n",
      "0.018462078645825386   \n",
      "0.00900366622954607   \n",
      "0.016365855932235718   \n",
      "0.012231294997036457   \n",
      "0.013798055239021778   \n",
      "0.012496388517320156   \n",
      "0.014736573211848736   \n",
      "0.009645896963775158   \n",
      "0.011497794650495052   \n",
      "0.011217431165277958   \n",
      "0.012819860130548477   \n",
      "0.013001885265111923   \n",
      "0.014512365683913231   \n",
      "0.011815190315246582   \n",
      "0.011188903823494911   \n",
      "0.010790792293846607   \n",
      "0.009313796646893024   \n",
      "0.011330007575452328   \n",
      "0.021696005016565323   \n",
      "0.01388576626777649   \n",
      "0.01122203841805458   \n",
      "0.008712999522686005   \n",
      "0.009283531457185745   \n",
      "0.00942322239279747   \n",
      "0.016405293717980385   \n",
      "0.011933178640902042   \n",
      "0.018379153683781624   \n",
      "0.014200867153704166   \n",
      "0.011823656968772411   \n",
      "0.01528375968337059   \n",
      "0.013400447554886341   \n",
      "0.01700359210371971   \n",
      "0.014088238589465618   \n",
      "0.012337223626673222   \n",
      "0.011000276543200016   \n",
      "0.01363619975745678   \n",
      "0.018405018374323845   \n",
      "0.01809603162109852   \n",
      "0.011377211660146713   \n",
      "0.013954604975879192   \n",
      "0.010580656118690968   \n",
      "0.011210215277969837   \n",
      "0.011305879801511765   \n",
      "0.010775853879749775   \n",
      "0.012357847765088081   \n",
      "0.01573323830962181   \n",
      "0.01094835251569748   \n",
      "0.011352147907018661   \n",
      "0.010895179584622383   \n",
      "0.008607536554336548   \n",
      "0.008918232284486294   \n",
      "0.015330389142036438   \n",
      "0.011388585902750492   \n",
      "0.014030028134584427   \n",
      "0.011439126916229725   \n",
      "0.009046380408108234   \n",
      "0.014037778601050377   \n",
      "0.013212211430072784   \n",
      "0.011858419515192509   \n",
      "0.0149778937920928   \n",
      "0.016710590571165085   \n",
      "0.015520519576966763   \n",
      "0.011248758062720299   \n",
      "0.012273798696696758   \n",
      "0.011487348936498165   \n",
      "0.016290483996272087   \n",
      "0.010911493562161922   \n",
      "0.01033862680196762   \n",
      "0.00981121975928545   \n",
      "0.01843089424073696   \n",
      "0.013751644641160965   \n",
      "0.011552773416042328   \n",
      "0.014130781404674053   \n",
      "0.010776612907648087   \n",
      "0.014614200219511986   \n",
      "0.01436750590801239   \n",
      "0.009980268776416779   \n",
      "0.01017005369067192   \n",
      "0.01240391843020916   \n",
      "0.011637750081717968   \n",
      "0.014520280063152313   \n",
      "0.009827596135437489   \n",
      "0.015026449225842953   \n",
      "0.012855522334575653   \n",
      "0.017154695466160774   \n",
      "0.01528649777173996   \n",
      "0.016170397400856018   \n",
      "0.014339211396872997   \n",
      "0.011583348736166954   \n",
      "0.014236909337341785   \n",
      "0.012776847928762436   \n",
      "0.013851208612322807   \n",
      "0.016761794686317444   \n",
      "0.01475366298109293   \n",
      "0.008824571967124939   \n",
      "0.02062194049358368   \n",
      "0.013388077728450298   \n",
      "0.014293177984654903   \n",
      "0.01052598562091589   \n",
      "0.010258855298161507   \n",
      "0.010531122796237469   \n",
      "0.014861551113426685   \n",
      "0.013090790249407291   \n",
      "0.014945868402719498   \n",
      "0.01150390412658453   \n",
      "0.011622773483395576   \n",
      "0.014239764772355556   \n",
      "0.012994756922125816   \n",
      "0.013612235896289349   \n",
      "0.010325810872018337   \n",
      "0.012066125869750977   \n",
      "0.015071042813360691   \n",
      "0.012080742046236992   \n",
      "0.010164482519030571   \n",
      "0.014499695971608162   \n",
      "0.013930609449744225   \n",
      "0.007497822865843773   \n",
      "0.01229010708630085   \n",
      "0.008837725967168808   \n",
      "0.0103529691696167   \n",
      "0.013934759423136711   \n",
      "0.012345926836133003   \n",
      "0.010822072625160217   \n",
      "0.011127517558634281   \n",
      "0.013884973712265491   \n",
      "0.01516946591436863   \n",
      "0.006263693794608116   \n",
      "0.020380990579724312   \n",
      "0.014194135554134846   \n",
      "0.009367397986352444   \n",
      "0.015025111846625805   \n",
      "0.010521826334297657   \n",
      "0.012274987064301968   \n",
      "0.012620918452739716   \n",
      "0.013691019266843796   \n",
      "0.013358170166611671   \n",
      "0.014757162891328335   \n",
      "0.010887186974287033   \n",
      "0.01024391409009695   \n",
      "0.01633075624704361   \n",
      "0.011652289889752865   \n",
      "0.008562922477722168   \n",
      "0.012090890668332577   \n",
      "0.010967151261866093   \n",
      "0.012212813831865788   \n",
      "0.019029289484024048   \n",
      "0.011629898101091385   \n",
      "0.017695767804980278   \n",
      "0.01032180618494749   \n",
      "0.007835309952497482   \n",
      "0.008774809539318085   \n",
      "0.00894229020923376   \n",
      "0.013618907891213894   \n",
      "0.009008092805743217   \n",
      "0.008831257000565529   \n",
      "0.00974938552826643   \n",
      "0.007408379577100277   \n",
      "0.01344449445605278   \n",
      "0.0067833601497113705   \n",
      "0.00631118007004261   \n",
      "0.011553319171071053   \n",
      "0.014598555862903595   \n",
      "0.009611251763999462   \n",
      "0.010464288294315338   \n",
      "0.013575474731624126   \n",
      "0.011320958845317364   \n",
      "0.012611692771315575   \n",
      "0.012041051872074604   \n",
      "0.010905193164944649   \n",
      "0.010096902959048748   \n",
      "0.008160488680005074   \n",
      "0.013045317493379116   \n",
      "0.01230892725288868   \n",
      "0.009894009679555893   \n",
      "0.011494698002934456   \n",
      "0.017360778525471687   \n",
      "0.019113814458251   \n",
      "0.016734620556235313   \n",
      "0.017515476793050766   \n",
      "0.01291454304009676   \n",
      "0.010873640887439251   \n",
      "0.013431185856461525   \n",
      "0.009714705869555473   \n",
      "0.013509315438568592   \n",
      "0.009138085879385471   \n",
      "0.014136075042188168   \n",
      "0.011827437207102776   \n",
      "0.010412518866360188   \n",
      "0.011690627783536911   \n",
      "0.009600101038813591   \n",
      "0.009360083378851414   \n",
      "0.01304260827600956   \n",
      "0.018657604232430458   \n",
      "0.01743949204683304   \n",
      "0.015150563791394234   \n",
      "0.012293496169149876   \n",
      "0.008474323898553848   \n",
      "0.010850932449102402   \n",
      "0.011498290114104748   \n",
      "0.009148715995252132   \n",
      "0.013423590920865536   \n",
      "0.014786436222493649   \n",
      "0.013380802236497402   \n",
      "0.009691363200545311   \n",
      "0.013387646526098251   \n",
      "0.00918902549892664   \n",
      "0.012397704645991325   \n",
      "0.010462945327162743   \n",
      "0.00939579214900732   \n",
      "0.010785396210849285   \n",
      "0.0071151829324662685   \n",
      "0.008329098112881184   \n",
      "0.010373755358159542   \n",
      "0.008563480339944363   \n",
      "0.01183586847037077   \n",
      "0.020271003246307373   \n",
      "0.011599971912801266   \n",
      "0.013131759129464626   \n",
      "0.01299266517162323   \n",
      "0.007477086968719959   \n",
      "0.010194335132837296   \n",
      "0.009460886009037495   \n",
      "0.010549911297857761   \n",
      "0.012264669872820377   \n",
      "0.007426910568028688   \n",
      "0.014721008017659187   \n",
      "0.010982519946992397   \n",
      "0.015258649364113808   \n",
      "0.01172726508229971   \n",
      "0.009184214286506176   \n",
      "0.012441926635801792   \n",
      "0.009168093092739582   \n",
      "0.008627261966466904   \n",
      "0.010508080944418907   \n",
      "0.017789745703339577   \n",
      "0.01724804937839508   \n",
      "0.014883291907608509   \n",
      "0.01487667579203844   \n",
      "0.010139117017388344   \n",
      "0.008758976124227047   \n",
      "0.015250145457684994   \n",
      "0.014641408808529377   \n",
      "0.012305314652621746   \n",
      "0.01021351758390665   \n",
      "0.012984146364033222   \n",
      "0.013962461613118649   \n",
      "0.01672230288386345   \n",
      "0.010166626423597336   \n",
      "0.008980857208371162   \n",
      "0.0171341635286808   \n",
      "0.009164640679955482   \n",
      "0.015566172078251839   \n",
      "0.012901373207569122   \n",
      "0.010425403714179993   \n",
      "0.009365840815007687   \n",
      "0.009746061637997627   \n",
      "0.0109994662925601   \n",
      "0.011303949169814587   \n",
      "0.01193233858793974   \n",
      "0.010081465356051922   \n",
      "0.014562557451426983   \n",
      "0.0082186758518219   \n",
      "0.020712245255708694   \n",
      "0.010828081518411636   \n",
      "0.012489059939980507   \n",
      "0.00956927239894867   \n",
      "0.010417794808745384   \n",
      "0.00909649021923542   \n",
      "0.01742432825267315   \n",
      "0.014709165319800377   \n",
      "0.006807105150073767   \n",
      "0.011391634121537209   \n",
      "0.011273276060819626   \n",
      "0.014100897125899792   \n",
      "0.009426734410226345   \n",
      "0.01448238268494606   \n",
      "0.011440950445830822   \n",
      "0.010728892870247364   \n",
      "0.011269571259617805   \n",
      "0.010220945812761784   \n",
      "0.010843232274055481   \n",
      "0.012781783938407898   \n",
      "0.00896994024515152   \n",
      "0.013930190354585648   \n",
      "0.010574279353022575   \n",
      "0.0084539158269763   \n",
      "0.015454904176294804   \n",
      "0.012027081102132797   \n",
      "0.012693230994045734   \n",
      "0.00891836266964674   \n",
      "0.01821773871779442   \n",
      "0.00817173346877098   \n",
      "0.00687287375330925   \n",
      "0.008424939587712288   \n",
      "0.010200466960668564   \n",
      "0.011933921836316586   \n",
      "0.011812225915491581   \n",
      "0.012190698646008968   \n",
      "0.009465181268751621   \n",
      "0.01148983370512724   \n",
      "0.013248099014163017   \n",
      "0.011073637753725052   \n",
      "0.009682121686637402   \n",
      "0.012819617986679077   \n",
      "0.018235361203551292   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011412256397306919   \n",
      "0.009716465137898922   \n",
      "0.011013743467628956   \n",
      "0.012938193045556545   \n",
      "0.011845782399177551   \n",
      "0.017424223944544792   \n",
      "0.015848226845264435   \n",
      "0.011568063870072365   \n",
      "0.00927746668457985   \n",
      "0.010722549632191658   \n",
      "0.009423954412341118   \n",
      "0.0110224150121212   \n",
      "0.013058441691100597   \n",
      "0.009189977310597897   \n",
      "0.008934439159929752   \n",
      "0.01279452908784151   \n",
      "0.011164965108036995   \n",
      "0.021292496472597122   \n",
      "0.016416218131780624   \n",
      "0.015629546716809273   \n",
      "0.011821306310594082   \n",
      "0.011355185881257057   \n",
      "0.009776846505701542   \n",
      "0.010446428321301937   \n",
      "0.013928921893239021   \n",
      "0.012903185561299324   \n",
      "0.018853118643164635   \n",
      "0.018846584483981133   \n",
      "0.009565931744873524   \n",
      "0.008369894698262215   \n",
      "0.008496559225022793   \n",
      "0.011136000044643879   \n",
      "0.012922286055982113   \n",
      "0.014183850958943367   \n",
      "0.01850060373544693   \n",
      "0.011567057110369205   \n",
      "0.008677320554852486   \n",
      "0.014141114428639412   \n",
      "0.015523144043982029   \n",
      "0.011527773924171925   \n",
      "0.00902960542589426   \n",
      "0.014668351039290428   \n",
      "0.011983571574091911   \n",
      "0.0163884237408638   \n",
      "0.01720605418086052   \n",
      "0.00757189467549324   \n",
      "0.010223940014839172   \n",
      "0.013069860637187958   \n",
      "0.014526375569403172   \n",
      "0.011227989569306374   \n",
      "0.011426227167248726   \n",
      "0.012229444459080696   \n",
      "0.012383921071887016   \n",
      "0.007516603916883469   \n",
      "0.011689476668834686   \n",
      "0.009586028754711151   \n",
      "0.011608180589973927   \n",
      "0.010938742198050022   \n",
      "0.01610906794667244   \n",
      "0.007657945156097412   \n",
      "0.012204647995531559   \n",
      "0.012761788442730904   \n",
      "0.009854541160166264   \n",
      "0.00919959507882595   \n",
      "0.00966282282024622   \n",
      "0.011229733005166054   \n",
      "0.012246709316968918   \n",
      "0.011917038820683956   \n",
      "0.009257753379642963   \n",
      "0.008878801949322224   \n",
      "0.012902569025754929   \n",
      "0.011164342984557152   \n",
      "0.01164332590997219   \n",
      "0.007466488517820835   \n",
      "0.007408726494759321   \n",
      "0.00816008634865284   \n",
      "0.008560648187994957   \n",
      "0.012424862012267113   \n",
      "0.009383969940245152   \n",
      "0.014778463169932365   \n",
      "0.010529730468988419   \n",
      "0.011557869613170624   \n",
      "0.015159162692725658   \n",
      "0.008157231844961643   \n",
      "0.03248819336295128   \n",
      "0.01013009063899517   \n",
      "0.012032932601869106   \n",
      "0.01274190004914999   \n",
      "0.009403749369084835   \n",
      "0.010453716851770878   \n",
      "0.01081029511988163   \n",
      "0.011356048285961151   \n",
      "0.015413621440529823   \n",
      "0.010915087535977364   \n",
      "0.010539681650698185   \n",
      "0.013352232985198498   \n",
      "0.011806528083980083   \n",
      "0.013603286817669868   \n",
      "0.01093775313347578   \n",
      "0.011980824172496796   \n",
      "0.015476414933800697   \n",
      "0.007101807743310928   \n",
      "0.006293070036917925   \n",
      "0.011804303154349327   \n",
      "0.009309696964919567   \n",
      "0.011140302754938602   \n",
      "0.01142130047082901   \n",
      "0.008779769763350487   \n",
      "0.01094985194504261   \n",
      "0.010032935068011284   \n",
      "0.007802325300872326   \n",
      "0.00786085519939661   \n",
      "0.01049523800611496   \n",
      "0.009251539595425129   \n",
      "0.01246670726686716   \n",
      "0.012001197785139084   \n",
      "0.008564022369682789   \n",
      "0.009200451895594597   \n",
      "0.015947910025715828   \n",
      "0.010610110126435757   \n",
      "0.011484034359455109   \n",
      "0.010216675698757172   \n",
      "0.01320920791476965   \n",
      "0.016485275700688362   \n",
      "0.011288536712527275   \n",
      "0.009639864787459373   \n",
      "0.00851333700120449   \n",
      "0.010780283249914646   \n",
      "0.021518193185329437   \n",
      "0.011652272194623947   \n",
      "0.011517204344272614   \n",
      "0.008265852928161621   \n",
      "0.008902166038751602   \n",
      "0.010260377079248428   \n",
      "0.012405862100422382   \n",
      "0.011498739942908287   \n",
      "0.011861257255077362   \n",
      "0.007800539955496788   \n",
      "0.008482595905661583   \n",
      "0.009003974497318268   \n",
      "0.00896905642002821   \n",
      "0.009523008018732071   \n",
      "0.01098702009767294   \n",
      "0.010358253493905067   \n",
      "0.012304019182920456   \n",
      "0.009381801821291447   \n",
      "0.008646831847727299   \n",
      "0.014191812835633755   \n",
      "0.008024435490369797   \n",
      "0.009168285876512527   \n",
      "0.010393221862614155   \n",
      "0.01379572693258524   \n",
      "0.012408374808728695   \n",
      "0.011254127137362957   \n",
      "0.009156397543847561   \n",
      "0.007111412938684225   \n",
      "0.01137750968337059   \n",
      "0.01989913545548916   \n",
      "0.012013844214379787   \n",
      "0.011294892057776451   \n",
      "0.012264372780919075   \n",
      "0.009555687196552753   \n",
      "0.009024660103023052   \n",
      "0.008758550509810448   \n",
      "0.0077869100496172905   \n",
      "0.009437959641218185   \n",
      "0.007436877582222223   \n",
      "0.013023239560425282   \n",
      "0.011950858868658543   \n",
      "0.009902372024953365   \n",
      "0.007985001429915428   \n",
      "0.013446250930428505   \n",
      "0.009430710226297379   \n",
      "0.012196999974548817   \n",
      "0.011246924288570881   \n",
      "0.008272102102637291   \n",
      "0.008839077316224575   \n",
      "0.008837118744850159   \n",
      "0.009668204933404922   \n",
      "0.01063468400388956   \n",
      "0.010619289241731167   \n",
      "0.009134605526924133   \n",
      "0.009955025278031826   \n",
      "0.011384084820747375   \n",
      "0.01651155762374401   \n",
      "0.00893604289740324   \n",
      "0.010941176675260067   \n",
      "0.01760086417198181   \n",
      "0.009475135244429111   \n",
      "0.012052363716065884   \n",
      "0.010461345314979553   \n",
      "0.011449980549514294   \n",
      "0.013957117684185505   \n",
      "0.009488929994404316   \n",
      "0.013057476840913296   \n",
      "0.009113925509154797   \n",
      "0.010540912859141827   \n",
      "0.006291380617767572   \n",
      "0.011728706769645214   \n",
      "0.01156071200966835   \n",
      "0.006326349917799234   \n",
      "0.01061191689223051   \n",
      "0.008071593940258026   \n",
      "0.00956936739385128   \n",
      "0.01128331944346428   \n",
      "0.012106209062039852   \n",
      "0.009064093232154846   \n",
      "0.009674492292106152   \n",
      "0.015010826289653778   \n",
      "0.012279270216822624   \n",
      "0.010802818462252617   \n",
      "0.009312724694609642   \n",
      "0.008262981660664082   \n",
      "0.005433263722807169   \n",
      "0.007578074466437101   \n",
      "0.013701537624001503   \n",
      "0.013559650629758835   \n",
      "0.012819220311939716   \n",
      "0.009452030062675476   \n",
      "0.009976149536669254   \n",
      "0.00996279064565897   \n",
      "0.010895239189267159   \n",
      "0.008592535741627216   \n",
      "0.007850416004657745   \n",
      "0.008016832172870636   \n",
      "0.008393514901399612   \n",
      "0.008117039687931538   \n",
      "0.009447470307350159   \n",
      "0.007380656898021698   \n",
      "0.009903643280267715   \n",
      "0.01531666237860918   \n",
      "0.011639679782092571   \n",
      "0.0075941141694784164   \n",
      "0.014095566235482693   \n",
      "0.008944512344896793   \n",
      "0.016260161995887756   \n",
      "0.011866336688399315   \n",
      "0.013897428289055824   \n",
      "0.01167974527925253   \n",
      "0.01177419163286686   \n",
      "0.011795730330049992   \n",
      "0.009501109831035137   \n",
      "0.010867636650800705   \n",
      "0.008138718083500862   \n",
      "0.010406652465462685   \n",
      "0.014927915297448635   \n",
      "0.008018430322408676   \n",
      "0.013361584395170212   \n",
      "0.006457434967160225   \n",
      "0.013015267439186573   \n",
      "0.007556827273219824   \n",
      "0.007342868484556675   \n",
      "0.00690148351714015   \n",
      "0.011126631870865822   \n",
      "0.009496641345322132   \n",
      "0.009251050651073456   \n",
      "0.008857369422912598   \n",
      "0.012899475172162056   \n",
      "0.008308219723403454   \n",
      "0.008252719417214394   \n",
      "0.015687115490436554   \n",
      "0.009249496273696423   \n",
      "0.008097791112959385   \n",
      "0.007416584063321352   \n",
      "0.011131778359413147   \n",
      "0.011955732479691505   \n",
      "0.009799137711524963   \n",
      "0.007925392128527164   \n",
      "0.011209051124751568   \n",
      "0.006628626491874456   \n",
      "0.010033600963652134   \n",
      "0.008268407545983791   \n",
      "0.00867532379925251   \n",
      "0.007714070379734039   \n",
      "0.011110964231193066   \n",
      "0.019764358177781105   \n",
      "0.012251077219843864   \n",
      "0.010196439921855927   \n",
      "0.00988833885639906   \n",
      "0.00949614867568016   \n",
      "0.012371961958706379   \n",
      "0.007836446166038513   \n",
      "0.008081931620836258   \n",
      "0.016318324953317642   \n",
      "0.010780702345073223   \n",
      "0.007709469646215439   \n",
      "0.008386588655412197   \n",
      "0.008986074477434158   \n",
      "0.009787319228053093   \n",
      "0.00820077583193779   \n",
      "0.008828486315906048   \n",
      "0.010921991430222988   \n",
      "0.014063265174627304   \n",
      "0.016469845548272133   \n",
      "0.012202010490000248   \n",
      "0.009386741556227207   \n",
      "0.015673533082008362   \n",
      "0.011380775831639767   \n",
      "0.009105795063078403   \n",
      "0.013366661965847015   \n",
      "0.010712939314544201   \n",
      "0.011629006825387478   \n",
      "0.009781966917216778   \n",
      "0.012692726217210293   \n",
      "0.010342384688556194   \n",
      "0.01727459207177162   \n",
      "0.01628919132053852   \n",
      "0.011393134482204914   \n",
      "0.009334680624306202   \n",
      "0.013409808278083801   \n",
      "0.011238405480980873   \n",
      "0.012181195430457592   \n",
      "0.015222212299704552   \n",
      "0.012243548408150673   \n",
      "0.009091576561331749   \n",
      "0.0086288470774889   \n",
      "0.014310785569250584   \n",
      "0.010552916675806046   \n",
      "0.012179478071630001   \n",
      "0.009495895355939865   \n",
      "0.009339925833046436   \n",
      "0.011189584620296955   \n",
      "0.008295558393001556   \n",
      "0.007169745396822691   \n",
      "0.01550353318452835   \n",
      "0.013008350506424904   \n",
      "0.015901293605566025   \n",
      "0.010218482464551926   \n",
      "0.010271698236465454   \n",
      "0.007145801559090614   \n",
      "0.011223873123526573   \n",
      "0.012656881473958492   \n",
      "0.009374387562274933   \n",
      "0.012451199814677238   \n",
      "0.009328029118478298   \n",
      "0.015777532011270523   \n",
      "0.011436080560088158   \n",
      "0.01226002536714077   \n",
      "0.012508375570178032   \n",
      "0.010516026988625526   \n",
      "0.015392107889056206   \n",
      "0.011359102092683315   \n",
      "0.009853214025497437   \n",
      "0.008844745345413685   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00686990050598979   \n",
      "0.010232379660010338   \n",
      "0.012048578821122646   \n",
      "0.009433319792151451   \n",
      "0.011265520006418228   \n",
      "0.007917865179479122   \n",
      "0.010329618118703365   \n",
      "0.01343134418129921   \n",
      "0.012784476391971111   \n",
      "0.00983603298664093   \n",
      "0.013675522990524769   \n",
      "0.010712148621678352   \n",
      "0.012072982266545296   \n",
      "0.014194424264132977   \n",
      "0.012227555736899376   \n",
      "0.010113265365362167   \n",
      "0.009963054209947586   \n",
      "0.013742635026574135   \n",
      "0.011592205613851547   \n",
      "0.009819810278713703   \n",
      "0.008122277446091175   \n",
      "0.01259947195649147   \n",
      "0.008532899431884289   \n",
      "0.013681931421160698   \n",
      "0.011103793978691101   \n",
      "0.008061419241130352   \n",
      "0.0089411661028862   \n",
      "0.011536496691405773   \n",
      "0.007883968763053417   \n",
      "0.009655972942709923   \n",
      "0.008786603808403015   \n",
      "0.014227470383048058   \n",
      "0.009009536355733871   \n",
      "0.011202111840248108   \n",
      "0.01058956515043974   \n",
      "0.021737905219197273   \n",
      "0.009955278597772121   \n",
      "0.011109883897006512   \n",
      "0.01463443972170353   \n",
      "0.01678716577589512   \n",
      "0.009893014095723629   \n",
      "0.011548561975359917   \n",
      "0.007910821586847305   \n",
      "0.00961941946297884   \n",
      "0.007508662063628435   \n",
      "0.017054973170161247   \n",
      "0.012708289548754692   \n",
      "0.01382491271942854   \n",
      "0.010947741568088531   \n",
      "0.011058229021728039   \n",
      "0.012173815630376339   \n",
      "0.008190504275262356   \n",
      "0.012581055983901024   \n",
      "0.00940537080168724   \n",
      "0.008395527489483356   \n",
      "0.015196933411061764   \n",
      "0.01289855595678091   \n",
      "0.011985263787209988   \n",
      "0.011218280531466007   \n",
      "0.01006392203271389   \n",
      "0.008447717875242233   \n",
      "0.009152347221970558   \n",
      "0.012642871588468552   \n",
      "0.013354454189538956   \n",
      "0.009128071367740631   \n",
      "0.011269886046648026   \n",
      "0.010172111913561821   \n",
      "0.015512725338339806   \n",
      "0.009843145497143269   \n",
      "0.011904201470315456   \n",
      "0.008262685500085354   \n",
      "0.008323363959789276   \n",
      "0.009914495050907135   \n",
      "0.008635946549475193   \n",
      "0.009831869043409824   \n",
      "0.012634608894586563   \n",
      "0.010002471506595612   \n",
      "0.014217446558177471   \n",
      "0.009846153669059277   \n",
      "0.011207195930182934   \n",
      "0.010966041125357151   \n",
      "0.007428627926856279   \n",
      "0.015121259726583958   \n",
      "0.007551264017820358   \n",
      "0.01436544954776764   \n",
      "0.012143564410507679   \n",
      "0.011833560653030872   \n",
      "0.015558969229459763   \n",
      "0.00931931845843792   \n",
      "0.015093138441443443   \n",
      "0.008670669049024582   \n",
      "0.008961157873272896   \n",
      "0.006726193707436323   \n",
      "0.008072084747254848   \n",
      "0.010135652497410774   \n",
      "0.017029110342264175   \n",
      "0.012010731734335423   \n",
      "0.009781695902347565   \n",
      "0.012053034268319607   \n",
      "0.01940092071890831   \n",
      "0.011312484741210938   \n",
      "0.008588924072682858   \n",
      "0.006849102210253477   \n",
      "0.0074778408743441105   \n",
      "0.014046482741832733   \n",
      "0.013944254256784916   \n",
      "0.009389428421854973   \n",
      "0.013592381030321121   \n",
      "0.006819067057222128   \n",
      "0.010939115658402443   \n",
      "0.011053591966629028   \n",
      "0.010017731226980686   \n",
      "0.010144009254872799   \n",
      "0.009433044120669365   \n",
      "0.008148659020662308   \n",
      "0.011514184065163136   \n",
      "0.014776901341974735   \n",
      "0.011849499307572842   \n",
      "0.006506464909762144   \n",
      "0.011364434845745564   \n",
      "0.009988076984882355   \n",
      "0.007115786429494619   \n",
      "0.007107618264853954   \n",
      "0.009610471315681934   \n",
      "0.010491777211427689   \n",
      "0.01023224275559187   \n",
      "0.011009170673787594   \n",
      "0.010802013799548149   \n",
      "0.007841041311621666   \n",
      "0.010527200996875763   \n",
      "0.017598534002900124   \n",
      "0.015725331380963326   \n",
      "0.009255899116396904   \n",
      "0.008964305743575096   \n",
      "0.009670869447290897   \n",
      "0.012213382869958878   \n",
      "0.010852464474737644   \n",
      "0.009176659397780895   \n",
      "0.014632871374487877   \n",
      "0.014657151885330677   \n",
      "0.013952442444860935   \n",
      "0.010200182907283306   \n",
      "0.010993165895342827   \n",
      "0.010286913253366947   \n",
      "0.00806127954274416   \n",
      "0.011943496763706207   \n",
      "0.008439796045422554   \n",
      "0.009608847089111805   \n",
      "0.010703351348638535   \n",
      "0.013218177482485771   \n",
      "0.008088497444987297   \n",
      "0.009603217244148254   \n",
      "0.011522194370627403   \n",
      "0.015122768469154835   \n",
      "0.00989356730133295   \n",
      "0.00823333952575922   \n",
      "0.011780310422182083   \n",
      "0.01123757753521204   \n",
      "0.00552197964861989   \n",
      "0.008170698769390583   \n",
      "0.009205913171172142   \n",
      "0.016985278576612473   \n",
      "0.01083170622587204   \n",
      "0.006696232594549656   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = '/tmp/NC2019MRI/train'\n",
    "    data_path_val = '/tmp/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    lr = 6e-3\n",
    "    \n",
    "    network = UNet(1,1)\n",
    "    network.to('cuda:0') #move the model on the GPU\n",
    "    mse_loss = nn.MSELoss().to('cuda:0')\n",
    "    \n",
    "    optimizer = optim.Adagrad(network.parameters(), lr=lr)\n",
    "    \n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=5, num_workers=num_workers) \n",
    "    \n",
    "\n",
    "    for epoch in range(2):\n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "        \n",
    "            img_gt, img_und = sample\n",
    "        \n",
    "            img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "            img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "\n",
    "            output = network(img_und)       #feedforward\n",
    "            #output = output.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "            loss = mse_loss(output, img_gt)\n",
    "            #loss = torch.tensor(ssim(img_gt, output)).to('cuda:0')\n",
    "            #print(loss.item())\n",
    "            optimizer.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer.step()     #update the weights\n",
    "\n",
    "            print(loss.item(), \"  \")\n",
    "        \n",
    "\n",
    "        \n",
    "        # stack different slices into a volume for visualisation\n",
    "#         A = masks[...,0].squeeze()\n",
    "#         B = torch.log(T.complex_abs(rawdata_und) + 1e-9).squeeze()\n",
    "#         C = T.complex_abs(img_und).squeeze()\n",
    "#         D = T.complex_abs(img_gt).squeeze()\n",
    "#         all_imgs = torch.stack([A,B,C,D], dim=0)\n",
    "\n",
    "#         # from left to right: mask, masked kspace, undersampled image, ground truth\n",
    "#         show_slices(all_imgs, [0, 1, 2, 3], cmap='gray')\n",
    "#         plt.pause(1)\n",
    "\n",
    "#         if iteration >= 0: break  # show 4 random slices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4205639660358429\n",
      "0.40286433696746826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2134"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, gt = train_dataset[6]\n",
    "image = image.unsqueeze(0).to('cuda:0')\n",
    "image = image.unsqueeze(0)\n",
    "#gt = gt.unsqueeze(0).to('cuda:0')\n",
    "gt = gt.unsqueeze(0).numpy()\n",
    "output = network(image)\n",
    "output = output.squeeze(1).cpu().detach().numpy()\n",
    "loss = torch.tensor(ssim(gt, output))\n",
    "loss2 = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "print(loss.item())\n",
    "print(loss2.item())\n",
    "#loss2 = mse_loss(output, gt)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058093959928229244\n",
      "0.4255652363152848\n"
     ]
    }
   ],
   "source": [
    "e = []\n",
    "a=[]\n",
    "b=[]\n",
    "i = 0\n",
    "for i in range(0,len(train_dataset)):\n",
    "    image, gt = train_dataset[i]\n",
    "    image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    #gt = gt.unsqueeze(0).to('cuda:0')\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network(image)\n",
    "    output = output.squeeze(1).cpu().detach().numpy()\n",
    "    loss = torch.tensor(ssim(gt, output))\n",
    "    loss2 = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    e.append(loss.item()-loss2.item())\n",
    "    a.append(loss.item())\n",
    "    b.append(loss2.item())\n",
    "    if loss.item()-loss2.item() < 0:\n",
    "        i+=1\n",
    "print(np.nanmean(e))\n",
    "print(np.nanmean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007775604259222746\n"
     ]
    }
   ],
   "source": [
    "image, gt = train_dataset[3]\n",
    "image = image.unsqueeze(0).to('cuda:0')\n",
    "image = image.unsqueeze(0)\n",
    "gt = gt.unsqueeze(0).to('cuda:0')\n",
    "gt = gt.unsqueeze(0)\n",
    "output = network(image)\n",
    "loss2 = mse_loss(output, gt)\n",
    "print(loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 8\n",
    "cen_fract = 0.04\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = '/tmp/NC2019MRI/train'\n",
    "    data_path_val = '/tmp/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    # create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "\n",
    "    a = [[],[]]\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, rawdata_und, masks, norm = sample\n",
    "        img_gt = T.center_crop(T.complex_abs(img_gt), [320, 320]).unsqueeze(1)\n",
    "        img_und = T.center_crop(T.complex_abs(img_und), [320, 320]).unsqueeze(1)\n",
    "        a[0].append(img_und)\n",
    "        a[1].append(img_gt)\n",
    "    b = torch.cat(a[0][:])\n",
    "    c = torch.cat(a[1][:])\n",
    "train = torch.stack((b,c),dim=0)\n",
    "del a\n",
    "del b\n",
    "del c\n",
    "del train_loader\n",
    "del train_dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b26a81dc5f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawdata_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m#feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "    \n",
    "network = AlexNet()\n",
    "network.to('cuda:0') #move the model on the GPU\n",
    "mse_loss = nn.MSELoss().to('cuda:0')\n",
    "    \n",
    "optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "for iteration, sample in enumerate(train_loader):\n",
    "    #img_gt, img_und, rawdata_und, masks, norm = sample        \n",
    "    \n",
    "    output = network(img_und)       #feedforward\n",
    "    print(output.shape)\n",
    "\n",
    "    loss = mse_loss(output, img_gt)\n",
    "    optimizer.zero_grad()       #set current gradients to 0\n",
    "    loss.backward()      #backpropagate\n",
    "    optimizer.step()     #update the weights\n",
    "    print(loss.item(), \"  \")\n",
    "        \n",
    "    i = 0\n",
    "    j +=1\n",
    "        \n",
    "    if j%100 == 0:\n",
    "        for row in range(0,320):\n",
    "            for col in range(0,320):\n",
    "                if output[0,0,row,col].item() == img_gt[0,0,row,col].item():\n",
    "\n",
    "                        i +=1\n",
    "        print(i, \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-33ad8426963b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2])\n",
    "a.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
